# ğŸ“Š Sistema FAVISA - AnÃ¡lisis de Contribuyentes Chimbote

**Universidad Nacional del Santa | Arquitectura de Software Empresarial | Grupo B | 2026**

DemostraciÃ³n acadÃ©mica de integraciÃ³n **SQL Server + Apache Spark + Python** usando **Docker**.

---

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCKER NETWORK: favisa_net               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     JDBC      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  SQL Server  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Python Orquestador â”‚    â”‚
â”‚  â”‚  :1433       â”‚               â”‚   (main.py)          â”‚    â”‚
â”‚  â”‚  FAVISA_DB   â”‚               â”‚   (spark_job.py)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   (dashboard.py)     â”‚    â”‚
â”‚         â”‚ pyodbc                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚ (carga CSV)                      â”‚ :8501          â”‚
â”‚         â”‚                                  â”‚ Streamlit UI   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                â”‚
â”‚  â”‚     Apache Spark Cluster      â”‚        â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”‚  â”‚  Master  â”‚ â”‚   Worker   â”‚  â”‚  spark-submit           â”‚
â”‚  â”‚  â”‚  :8080   â”‚ â”‚   :8081    â”‚  â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de ejecuciÃ³n:
1. **SQL Server** arranca y espera conexiones
2. **Python** espera a que SQL Server estÃ© listo (healthcheck)
3. **Python** ejecuta `init.sql` â†’ crea FAVISA_DB con todas las tablas
4. **Python** lee el CSV y lo carga via `pyodbc` (tabla staging)
5. **Python** limpia datos sucios y migra a tabla `CONTRIBUYENTE`
6. **PySpark** se conecta a SQL Server via JDBC y realiza 6 anÃ¡lisis
7. Los resultados se guardan en `RESULTADO_SPARK` y se muestran en consola
8. **Streamlit** lanza el dashboard web en el puerto 8501

---

## ğŸ“ Estructura del Proyecto

```
proyecto/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml    # OrquestaciÃ³n de contenedores
â”‚   â””â”€â”€ Dockerfile            # Imagen Python personalizada
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql              # Script BD completo (adaptado para Docker)
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ main.py               # Orquestador principal
â”‚   â”œâ”€â”€ spark_job.py          # AnÃ¡lisis con Apache Spark
â”‚   â”œâ”€â”€ db_connection.py      # MÃ³dulo de conexiÃ³n SQL Server
â”‚   â”œâ”€â”€ dashboard.py          # Dashboard Streamlit
â”‚   â””â”€â”€ requirements.txt      # Dependencias Python
â””â”€â”€ data/
    â””â”€â”€ DataOZ_ChimboteNuevoCampo_03.csv   # Datos de contribuyentes
```

---

## âœ… Requisitos Previos

Solo necesitas **Docker Desktop** instalado:

| Sistema | Link |
|---------|------|
| Windows | https://docs.docker.com/desktop/install/windows-install/ |
| Mac     | https://docs.docker.com/desktop/install/mac-install/ |
| Linux   | https://docs.docker.com/desktop/install/linux-install/ |

Verificar instalaciÃ³n:
```bash
docker --version
docker compose version
```

**Recursos recomendados para Docker Desktop:**
- RAM: mÃ­nimo 6 GB asignados a Docker (8 GB recomendado)
- CPU: 2 cores mÃ­nimo
- Disco: 10 GB libres

---

## ğŸš€ EjecuciÃ³n Paso a Paso

### 1. Clonar / Descomprimir el proyecto

```bash
# Si tienes git:
git clone <repositorio>

# O simplemente descomprimir el ZIP y entrar al directorio
cd proyecto
```

### 2. Ejecutar el sistema

```bash
# Desde la carpeta /docker (donde estÃ¡ el docker-compose.yml):
cd docker

# Construir imÃ¡genes y levantar todos los contenedores:
docker compose up --build
```

> â±ï¸ **Primera vez**: puede tardar 5-10 minutos descargando imÃ¡genes.
> Las siguientes ejecuciones tardan ~2 minutos.

### 3. Observar los logs

VerÃ¡s en consola algo asÃ­:

```
favisa_sqlserver  | SQL Server is now ready for client connections.
favisa_python     | [PASO 1] Verificando conexiÃ³n a SQL Server... âœ…
favisa_python     | [PASO 2] Ejecutando script SQL...
favisa_python     | [PASO 3] Cargando CSV... 50,100 filas OK
favisa_python     | [PASO 4] Poblando catÃ¡logos...
favisa_python     | [PASO 5] Migrando datos...
favisa_python     | [PASO 6] Ejecutando anÃ¡lisis Spark...
favisa_python     |   ANÃLISIS A: SALUD FISCAL DEL ECOSISTEMA
favisa_python     |   ANÃLISIS B: DISTRIBUCIÃ“N GEOGRÃFICA
...
favisa_python     | Dashboard disponible en: http://localhost:8501
```

### 4. Acceder a las interfaces

| Interfaz | URL | DescripciÃ³n |
|----------|-----|-------------|
| **Dashboard principal** | http://localhost:8501 | Streamlit con grÃ¡ficos y resultados |
| **Spark Master UI** | http://localhost:8080 | Estado del cluster Spark |
| **Spark Worker UI** | http://localhost:8081 | Tareas en ejecuciÃ³n |

---

## ğŸ”„ Re-ejecuciÃ³n

Para ejecutar nuevamente sin reconstruir:

```bash
docker compose down
docker compose up
```

Para reconstruir la imagen Python (si cambias requirements.txt o Dockerfile):

```bash
docker compose up --build
```

Para ver solo los logs de un servicio:

```bash
docker compose logs python-app
docker compose logs sqlserver
docker compose logs spark-master
```

---

## ğŸ—„ï¸ Conectarse a SQL Server desde SSMS / DBeaver

Si quieres explorar la base desde una herramienta externa:

| Campo | Valor |
|-------|-------|
| Servidor | `localhost,1433` |
| AutenticaciÃ³n | SQL Server |
| Usuario | `SA` |
| ContraseÃ±a | `FavisaDB2024!` |
| Base de datos | `FAVISA_DB` |

---

## ğŸ“Š AnÃ¡lisis Spark Incluidos

| AnÃ¡lisis | DescripciÃ³n | Relevancia FAVISA |
|----------|-------------|-------------------|
| **A. Salud Fiscal** | Estado tributario, deuda total/promedio/mÃ¡xima | Riesgo de proveedores |
| **B. GeografÃ­a** | Top 15 distritos por concentraciÃ³n | ExpansiÃ³n CHIC/Eventos |
| **C. Estructura** | TamaÃ±o y tipo de empresa | Perfil de competidores |
| **D. Sectores CIIU** | Top 20 + anÃ¡lisis sectores 52xx, 15xx, 70xx | Competencia directa |
| **E. DemografÃ­a** | Sexo, edad, rangos etarios | SegmentaciÃ³n de mercado |
| **F. Calidad** | Datos nulos, valores sucios normalizados | DiagnÃ³stico de datos |

---

## ğŸ”§ Troubleshooting

### "python-app se desconecta antes de que SQL Server estÃ© listo"
Normal en la primera ejecuciÃ³n. El sistema tiene reintentos automÃ¡ticos
(cada 5 segundos, hasta 2 minutos). SQL Server tarda ~45-60s en arrancar.

### "Error JDBC: driver not found"
El JAR JDBC se descarga durante el build del Dockerfile. Si falla la descarga
(sin internet), puedes descargarlo manualmente:
```
https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.4.2.jre11/mssql-jdbc-12.4.2.jre11.jar
```
Y colocarlo en: `/opt/spark-jars/` dentro del contenedor.

### "Puerto 1433 ya en uso"
Tienes SQL Server local corriendo. Cambia el port mapping en docker-compose.yml:
```yaml
ports:
  - "1434:1433"  # cambia el primer nÃºmero
```

### Ver quÃ© hay en la BD sin SSMS:
```bash
docker exec -it favisa_sqlserver /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U SA -P 'FavisaDB2024!' \
  -Q "SELECT COUNT(*) FROM FAVISA_DB.dbo.CONTRIBUYENTE" -No -C
```

---

## ğŸ›‘ Detener el sistema

```bash
# Detener contenedores (mantiene datos):
docker compose down

# Detener Y borrar todos los datos (limpieza total):
docker compose down -v
```

---

## ğŸ“ Notas de AdaptaciÃ³n del Script SQL

El script `init.sql` es el script original del equipo con estas adaptaciones mÃ­nimas:
1. **Se removiÃ³ `BULK INSERT`** â†’ La carga del CSV la hace Python (mÃ¡s portable)
2. **Se agregaron valores al catÃ¡logo** de estados y condiciones que aparecen en el CSV real
3. **Se creÃ³ tabla `RESULTADO_SPARK`** para persistir los anÃ¡lisis (no existÃ­a en original)
4. **Sin cambios en**: tablas, vistas, relaciones, Ã­ndices, constraints de la estructura original

---

*Proyecto acadÃ©mico - Universidad Nacional del Santa - 2026*
